{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13f7c88",
   "metadata": {},
   "source": [
    "See links for instructions on installation if not already installed.\n",
    " - [pandas](https://pypi.org/project/pandas/) 1.3.3\n",
    " - [nltk](https://www.nltk.org/install.html) 3.6.1\n",
    " - [spacy](https://spacy.io/usage) 2.2.2\n",
    "    (and download the pipeline with `en_core_web_sm`)\n",
    " - [tokenizers](https://pypi.org/project/tokenizers/) 0.10.3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690279a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f78ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('dataset/IRAhandle_tweets_1.csv')['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d24d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_stats(tokens):\n",
    "    '''\n",
    "    Prints out tokenization stats of the corpus\n",
    "    \n",
    "    Input:\n",
    "        tokens (list): a list of all tokens in the entire corpus\n",
    "    \n",
    "    Output:\n",
    "        - number of types (e.g., vocab size)\n",
    "        - number of tokens\n",
    "        - type/token ratio of all tweets\n",
    "        - top 10 tokens in terms of frequency in the vocabulary\n",
    "    '''\n",
    "    \n",
    "    num_type = len(set(tokens))\n",
    "    num_token = len(tokens)\n",
    "    ratio = num_type/num_token\n",
    "    \n",
    "    counter = Counter()\n",
    "    for token in tokens:\n",
    "        counter[token] += 1\n",
    "            \n",
    "    top_tokens = sorted(counter, key = counter.get, reverse = True)[:10]\n",
    "    \n",
    "    return num_type, num_token, ratio, top_tokens\n",
    "\n",
    "def tokenize_texts(corpus, tokenize_func, single_func):\n",
    "    '''\n",
    "    Input:\n",
    "        corpus (list): a list containing the contents of the IRAhandle_tweets_1.csv\n",
    "        tokenize_func (function): the function to be used for tokenizing the corpus\n",
    "        single_func (function): tokenization function for individual tweets\n",
    "        \n",
    "    Output:\n",
    "        - prints tokenization status using print_stats()\n",
    "        - prints tokenization results of the first 3 tweets of the corpus\n",
    "    '''\n",
    "    \n",
    "    tokens = tokenize_func(corpus)\n",
    "    \n",
    "    stats = print_stats(tokens)\n",
    "    print(\"The number of types is \", stats[0])\n",
    "    print(\"The number of tokens is \", stats[1])\n",
    "    print(\"The type/token ratio is \", stats[2])\n",
    "    print(\"The top 10 tokens in terms of frequency are \", stats[3])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"The tokenization results of the first tweet are:\\n\", single_func(corpus[0].lower()), \"\\n\")\n",
    "    print(\"The tokenization results of the second tweet are:\\n\", single_func(corpus[1].lower()), \"\\n\")\n",
    "    print(\"The tokenization results of the second tweet are:\\n\", single_func(corpus[2].lower()), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828c867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize by space:\n",
      "\n",
      "The number of types is  554069\n",
      "The number of tokens is  3267403\n",
      "The type/token ratio is  0.16957473565397352\n",
      "The top 10 tokens in terms of frequency are  ['the', 'to', 'a', 'in', 'of', 'is', 'for', 'and', 'Ð²', 'rt']\n",
      "\n",
      "\n",
      "The tokenization results of the first tweet are:\n",
      " ['\"we', 'have', 'a', 'sitting', 'democrat', 'us', 'senator', 'on', 'trial', 'for', 'corruption', 'and', \"you've\", 'barely', 'heard', 'a', 'peep', 'from', 'the', 'mainstream', 'media.\"', '~', '@nedryun', 'https://t.co/gh6g0d1oic'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['marshawn', 'lynch', 'arrives', 'to', 'game', 'in', 'anti-trump', 'shirt.', 'judging', 'by', 'his', 'sagging', 'pants', 'the', 'shirt', 'should', 'say', 'lynch', 'vs.', 'belt', 'https://t.co/mlh1i30lzz'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['daughter', 'of', 'fallen', 'navy', 'sailor', 'delivers', 'powerful', 'monologue', 'on', 'anthem', 'protests,', 'burns', 'her', 'nfl', 'packers', 'gear.', '#boycottnfl', 'https://t.co/qdlfbgmeag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_simple(text):\n",
    "    '''\n",
    "    A simple tokenizer\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweets from the corpus\n",
    "    \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    split_list = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        split_list += tweet.split()\n",
    "    \n",
    "    return [token.lower() for token in split_list]\n",
    "\n",
    "def single_simple(text):\n",
    "    '''\n",
    "    A simple tokenizer for individual tweets\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweet from the corpus\n",
    "        \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "print('Tokenize by space:\\n')\n",
    "tokenize_texts(corpus, tokenize_simple, single_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c920b",
   "metadata": {},
   "source": [
    "### NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adb11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize with WordPunctTokenizer:\n",
      "\n",
      "The number of types is  417701\n",
      "The number of tokens is  5605127\n",
      "The type/token ratio is  0.0745212374313731\n",
      "The top 10 tokens in terms of frequency are  ['.', 't', '/', 'co', '://', 'https', '#', 'the', ',', \"'\"]\n",
      "\n",
      "\n",
      "The tokenization results of the first tweet are:\n",
      " ['\"', 'we', 'have', 'a', 'sitting', 'democrat', 'us', 'senator', 'on', 'trial', 'for', 'corruption', 'and', 'you', \"'\", 've', 'barely', 'heard', 'a', 'peep', 'from', 'the', 'mainstream', 'media', '.\"', '~', '@', 'nedryun', 'https', '://', 't', '.', 'co', '/', 'gh6g0d1oic'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['marshawn', 'lynch', 'arrives', 'to', 'game', 'in', 'anti', '-', 'trump', 'shirt', '.', 'judging', 'by', 'his', 'sagging', 'pants', 'the', 'shirt', 'should', 'say', 'lynch', 'vs', '.', 'belt', 'https', '://', 't', '.', 'co', '/', 'mlh1i30lzz'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['daughter', 'of', 'fallen', 'navy', 'sailor', 'delivers', 'powerful', 'monologue', 'on', 'anthem', 'protests', ',', 'burns', 'her', 'nfl', 'packers', 'gear', '.', '#', 'boycottnfl', 'https', '://', 't', '.', 'co', '/', 'qdlfbgmeag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: write a short script, along with any necessary functions, to implement the NLTK tokenizer.\n",
    "# You can follow the example in 1.1, but you will need to think about the input parameters to the tokenizer function\n",
    "# You may find partial in functools a useful thing to use\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from functools import partial\n",
    "\n",
    "def tokenize_nltk_wpt(text):\n",
    "    '''\n",
    "    A tokenizer using the tokenize method of nltk WordPunctTokenizer\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweets from the corpus\n",
    "    \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    tk = WordPunctTokenizer()\n",
    "    split_list = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        split_list += tk.tokenize(tweet)\n",
    "\n",
    "    return [token.lower() for token in split_list]\n",
    "\n",
    "def wpt_single(text):\n",
    "    '''\n",
    "    An nltk tokenizer for individual tweets\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweet from the corpus\n",
    "        \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    tk = WordPunctTokenizer()\n",
    "    \n",
    "    return tk.tokenize(text)\n",
    "\n",
    "print(\"Tokenize with WordPunctTokenizer:\\n\")\n",
    "tokenize_texts(corpus, tokenize_nltk_wpt, wpt_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a0abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize with TreebankWordTokenizer:\n",
      "\n",
      "The number of types is  464905\n",
      "The number of tokens is  4367567\n",
      "The type/token ratio is  0.10644484675335261\n",
      "The top 10 tokens in terms of frequency are  [':', 'https', '#', 'the', ',', '@', 'to', '!', 'a', 'in']\n",
      "\n",
      "\n",
      "The tokenization results of the first tweet are:\n",
      " ['``', 'we', 'have', 'a', 'sitting', 'democrat', 'us', 'senator', 'on', 'trial', 'for', 'corruption', 'and', 'you', \"'ve\", 'barely', 'heard', 'a', 'peep', 'from', 'the', 'mainstream', 'media.', \"''\", '~', '@', 'nedryun', 'https', ':', '//t.co/gh6g0d1oic'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['marshawn', 'lynch', 'arrives', 'to', 'game', 'in', 'anti-trump', 'shirt.', 'judging', 'by', 'his', 'sagging', 'pants', 'the', 'shirt', 'should', 'say', 'lynch', 'vs.', 'belt', 'https', ':', '//t.co/mlh1i30lzz'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['daughter', 'of', 'fallen', 'navy', 'sailor', 'delivers', 'powerful', 'monologue', 'on', 'anthem', 'protests', ',', 'burns', 'her', 'nfl', 'packers', 'gear.', '#', 'boycottnfl', 'https', ':', '//t.co/qdlfbgmeag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def tokenize_nltk_twt(text):\n",
    "    '''\n",
    "    A tokenizer using the tokenize method of nltk TreebankWordTokenizer\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweets from the corpus\n",
    "    \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    tk = TreebankWordTokenizer()\n",
    "    split_list = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        split_list += tk.tokenize(tweet)\n",
    "\n",
    "    return [token.lower() for token in split_list]\n",
    "\n",
    "def twt_single(text):\n",
    "    '''\n",
    "    An nltk tokenizer for individual tweets\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweet from the corpus\n",
    "        \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    tk = TreebankWordTokenizer()\n",
    "    \n",
    "    return tk.tokenize(text)\n",
    "\n",
    "print(\"Tokenize with TreebankWordTokenizer:\\n\")\n",
    "tokenize_texts(corpus, tokenize_nltk_twt, twt_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d33b9",
   "metadata": {},
   "source": [
    "### spaCy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269de1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize with spaCy:\n",
      "\n",
      "The number of types is  427416\n",
      "The number of tokens is  4062629\n",
      "The type/token ratio is  0.10520675158868802\n",
      "The top 10 tokens in terms of frequency are  ['#', '.', 'the', ',', ':', 'to', ' ', '!', 'a', 'in']\n",
      "\n",
      "\n",
      "The tokenization results of the first tweet are:\n",
      " ['\"', 'we', 'have', 'a', 'sitting', 'democrat', 'us', 'senator', 'on', 'trial', 'for', 'corruption', 'and', 'you', \"'ve\", 'barely', 'heard', 'a', 'peep', 'from', 'the', 'mainstream', 'media', '.', '\"', '~', '@nedryun', 'https://t.co/gh6g0d1oic'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['marshawn', 'lynch', 'arrives', 'to', 'game', 'in', 'anti', '-', 'trump', 'shirt', '.', 'judging', 'by', 'his', 'sagging', 'pants', 'the', 'shirt', 'should', 'say', 'lynch', 'vs.', 'belt', 'https://t.co/mlh1i30lzz'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['daughter', 'of', 'fallen', 'navy', 'sailor', 'delivers', 'powerful', 'monologue', 'on', 'anthem', 'protests', ',', 'burns', 'her', 'nfl', 'packers', 'gear', '.', ' ', '#', 'boycottnfl', 'https://t.co/qdlfbgmeag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "def tokenize_spacy(text):\n",
    "    '''\n",
    "    A tokenizer using the spaCy tokenization toolkit\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweets from the corpus\n",
    "    \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    split_list = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        doc = nlp(tweet)\n",
    "        split_list += [token.text for token in doc]\n",
    "    \n",
    "    return [token.lower() for token in split_list]\n",
    "\n",
    "def spacy_single(text):\n",
    "    '''\n",
    "    A tokenizer using the spaCy tokenization toolkit for individual tweets\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweet from the corpus\n",
    "    \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return [token.text for token in doc]\n",
    "\n",
    "print(\"Tokenize with spaCy:\\n\")\n",
    "tokenize_texts(corpus, tokenize_spacy, spacy_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b237f",
   "metadata": {},
   "source": [
    "### BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902b70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('dataset/IRAhandle_tweets_1.csv', usecols = ['content'])\n",
    "train_data = train_data['content'].str.lower()\n",
    "train_data.to_csv('dataset/IRA_BPE_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2047282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train(files = ['dataset/IRA_BPE_train.csv'], trainer = trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b857889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize with BPE:\n",
      "\n",
      "The number of types is  27739\n",
      "The number of tokens is  7308449\n",
      "The type/token ratio is  0.0037954701469490996\n",
      "The top 10 tokens in terms of frequency are  ['.', 't', '/', 'co', '://', 'https', '#', ',', \"'\", ':']\n",
      "\n",
      "\n",
      "The tokenization results of the first tweet are:\n",
      " ['\"', 'we', 'have', 'a', 'sitting', 'democrat', 'us', 'senator', 'on', 'trial', 'for', 'corruption', 'and', 'you', \"'\", 've', 'barely', 'heard', 'a', 'peep', 'from', 'the', 'mainstream', 'media', '.\"', '~', '@', 'ned', 'ry', 'un', 'https', '://', 't', '.', 'co', '/', 'gh', '6', 'g0', 'd1', 'o', 'ic'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['mar', 'shawn', 'lynch', 'arrives', 'to', 'game', 'in', 'anti', '-', 'trump', 'shirt', '.', 'judging', 'by', 'his', 'sag', 'ging', 'pants', 'the', 'shirt', 'should', 'say', 'lynch', 'vs', '.', 'belt', 'https', '://', 't', '.', 'co', '/', 'ml', 'h1', 'i', '30', 'l', 'zz'] \n",
      "\n",
      "The tokenization results of the second tweet are:\n",
      " ['daughter', 'of', 'fallen', 'navy', 'sailor', 'delivers', 'powerful', 'mon', 'olo', 'gue', 'on', 'anthem', 'protests', ',', 'burns', 'her', 'nfl', 'pack', 'ers', 'gear', '.', '#', 'boycottnfl', 'https', '://', 't', '.', 'co', '/', 'qd', 'lf', 'bg', 'me', 'ag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_bpe(text):\n",
    "    '''\n",
    "    A tokenizer that implements the BPE tokenizer\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweets from the corpus\n",
    "        \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    split_list = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        output = tokenizer.encode(tweet)\n",
    "        split_list += output.tokens\n",
    "    \n",
    "    return [token.lower() for token in split_list]\n",
    "\n",
    "def bpe_single(text):\n",
    "    '''\n",
    "    A tokenizer that implements the BPE tokenizer for individual tweets\n",
    "    \n",
    "    Input:\n",
    "        text (string): tweet from the corpus\n",
    "        \n",
    "    Output: a list of tokens\n",
    "    '''\n",
    "    \n",
    "    output = tokenizer.encode(text)\n",
    "    return output.tokens\n",
    "\n",
    "print(\"Tokenize with BPE:\\n\")\n",
    "tokenize_texts(corpus, tokenize_bpe, bpe_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5f774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_env]",
   "language": "python",
   "name": "conda-env-nlp_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
