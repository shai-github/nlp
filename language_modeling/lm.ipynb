{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcWCOlx1vSmf"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDlJ3yEpvX4n",
    "outputId": "2c057701-016c-4c10-865c-2f931e83e4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "zyBV10nMJIrO"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./lm_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "j2IKVqkxzN1L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "7asRBEtT102s"
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "WORD_EMBED_DIM = 200\n",
    "HID_EMBED_DIM = 200 \n",
    "N_LAYERS = 2 \n",
    "DROPOUT = 0.5 \n",
    "LOG_INTERVAL = 100\n",
    "EPOCHS = 10\n",
    "BPTT = 50 # sequence length\n",
    "CLIP = 0.25\n",
    "TIED = False\n",
    "SAVE_BEST = os.path.join(DATA_DIR, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_45k6t7hzgsA"
   },
   "source": [
    "## Build vocabulary and convert text in corpus to lists of word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "xr3dkhUg2fiG"
   },
   "outputs": [],
   "source": [
    "class WordDict(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx.keys():\n",
    "          self.word2idx[word] = len(self.word2idx)\n",
    "          self.idx2word[len(self.idx2word)] = word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.train_file = os.path.join(path, 'train.txt')\n",
    "        self.valid_file = os.path.join(path, 'valid.txt')\n",
    "        self.test_file = os.path.join(path, 'test.txt')\n",
    "\n",
    "        self.dictionary = WordDict() \n",
    "\n",
    "        self.train = self.tokenize(self.train_file)\n",
    "        self.valid = self.tokenize(self.valid_file)\n",
    "        self.test = self.tokenize(self.test_file)\n",
    "                                   \n",
    "    def tokenize(self, filename):\n",
    "        f = open(filename).readlines()\n",
    "\n",
    "        token_lines = []\n",
    "        for line in f:\n",
    "          lower_line = line.lower()\n",
    "          split_line = lower_line.split()\n",
    "          if split_line != []:\n",
    "            split_line.insert(0, \"<sos>\")\n",
    "            split_line.append(\"<eos>\")\n",
    "            for token in split_line:\n",
    "              self.dictionary.add_word(token)\n",
    "              token_lines.append(self.dictionary.word2idx[token])\n",
    "\n",
    "        return token_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4gS0wC5DVb_",
    "outputId": "96982150-47bf-4a6b-a99d-7964fbf46b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099444\n",
      "218808\n",
      "246993\n",
      "28913\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(DATA_DIR)\n",
    "print(len(corpus.train))\n",
    "print(len(corpus.valid))\n",
    "print(len(corpus.test))\n",
    "print(len(corpus.dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "6of33pFT94dM"
   },
   "outputs": [],
   "source": [
    "def batchify(ids, batch_size):\n",
    "\n",
    "    id_tensor = torch.Tensor(ids)\n",
    "    id_tensor = id_tensor.to(torch.long)\n",
    "\n",
    "    id_tensor = id_tensor.view(id_tensor.size(0), -1)\n",
    "    id_tensor = id_tensor[:(len(ids)//100)*100]\n",
    "    id_tensor = torch.reshape(id_tensor, (batch_size, len(ids)//batch_size)).permute(1, 0)\n",
    "\n",
    "    return id_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOnfTvfjPW73",
    "outputId": "502654ed-8a77-43a7-aeb9-3f8f8928ece6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20994, 100])\n",
      "torch.Size([2188, 100])\n",
      "torch.Size([2469, 100])\n"
     ]
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, TRAIN_BATCH_SIZE)\n",
    "val_data = batchify(corpus.valid, TEST_BATCH_SIZE)\n",
    "test_data = batchify(corpus.test, TEST_BATCH_SIZE)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "qhtQEbhR_hNT"
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "\n",
    "    if i + BPTT >= len(source):\n",
    "      seq_len = len(source) - 1 - i\n",
    "    else:\n",
    "      seq_len = BPTT\n",
    "\n",
    "    data = source[i:(seq_len + i)]\n",
    "    target = torch.flatten(source[(i + 1):(seq_len + 1 + i)])\n",
    "\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNaCdII4DBLo",
    "outputId": "be2d2abd-333b-4542-af53-b958e2fc8a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
      "        [    1,  1791,    14,  ...,   438,  8623, 20553],\n",
      "        [    2,   130,   119,  ...,   984,    18,   300],\n",
      "        ...,\n",
      "        [   35,    17,  5419,  ...,  5099,    16,    14],\n",
      "        [   36,   346,    62,  ...,    14,     5,  1625],\n",
      "        [   37,  3544,    38,  ...,  7773,     0,  1654]])\n",
      "tensor([    1,  1791,    14,  ..., 17113,     1,  5407])\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 0)\n",
    "print(data)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "6C8QNsUL9i1S"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, word_embedding_size, nhid, nlayers, dropout=0.5, tied_weights=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.encoder = torch.nn.Embedding(vocab_size, word_embedding_size)\n",
    "        self.decoder = torch.nn.Linear(word_embedding_size, vocab_size)\n",
    "        self.nhid = nhid \n",
    "        self.nlayers = nlayers \n",
    "        self.lstm = torch.nn.LSTM(input_size = word_embedding_size, hidden_size = nhid, num_layers = nlayers, batch_first = False)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        self.encoder.weight = nn.init.uniform_(self.encoder.weight)\n",
    "        self.decoder.weight = nn.init.uniform_(self.decoder.weight)\n",
    "        self.encoder.weight.requires_grad = True\n",
    "        self.encoder.weight.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, hidden):\n",
    "\n",
    "        embeds = self.encoder(input_ids)\n",
    "        embeds = self.dropout(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        decoded = self.decoder(out)\n",
    "\n",
    "        return decoded.reshape((-1, decoded.shape[2])), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "hVpQfjuxOm0Y"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "xZT5ZAu7EX3v"
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(TRAIN_BATCH_SIZE)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        hidden = repackage_hidden(hidden) \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // BPTT,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "aTsdN6rLG0fc"
   },
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    hidden = model.init_hidden(TEST_BATCH_SIZE)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, BPTT)):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    average_log_loss = total_loss / batch\n",
    "\n",
    "    return average_log_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYuaQ5cSHxWI",
    "outputId": "f9dbd8a3-ad3e-4e38-8e15-9f606507ba1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  419 batches | ms/batch 56.70 | loss  8.18 | ppl  3573.28\n",
      "| epoch   1 |   200/  419 batches | ms/batch 55.02 | loss  6.93 | ppl  1024.00\n",
      "| epoch   1 |   300/  419 batches | ms/batch 55.10 | loss  6.72 | ppl   828.19\n",
      "| epoch   1 |   400/  419 batches | ms/batch 55.35 | loss  6.57 | ppl   716.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 24.05s | valid loss  6.18 | valid ppl   485.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   2 |   100/  419 batches | ms/batch 55.77 | loss  6.50 | ppl   662.40\n",
      "| epoch   2 |   200/  419 batches | ms/batch 54.96 | loss  6.33 | ppl   563.19\n",
      "| epoch   2 |   300/  419 batches | ms/batch 55.08 | loss  6.26 | ppl   520.85\n",
      "| epoch   2 |   400/  419 batches | ms/batch 55.52 | loss  6.18 | ppl   484.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 23.98s | valid loss  5.89 | valid ppl   360.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   3 |   100/  419 batches | ms/batch 56.02 | loss  6.19 | ppl   487.08\n",
      "| epoch   3 |   200/  419 batches | ms/batch 55.47 | loss  6.06 | ppl   430.43\n",
      "| epoch   3 |   300/  419 batches | ms/batch 55.50 | loss  6.00 | ppl   405.40\n",
      "| epoch   3 |   400/  419 batches | ms/batch 55.14 | loss  5.96 | ppl   386.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 24.05s | valid loss  5.71 | valid ppl   300.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   4 |   100/  419 batches | ms/batch 55.60 | loss  5.99 | ppl   400.03\n",
      "| epoch   4 |   200/  419 batches | ms/batch 55.06 | loss  5.89 | ppl   361.75\n",
      "| epoch   4 |   300/  419 batches | ms/batch 55.06 | loss  5.84 | ppl   343.31\n",
      "| epoch   4 |   400/  419 batches | ms/batch 55.31 | loss  5.80 | ppl   331.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 23.95s | valid loss  5.59 | valid ppl   268.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   5 |   100/  419 batches | ms/batch 56.04 | loss  5.85 | ppl   347.01\n",
      "| epoch   5 |   200/  419 batches | ms/batch 55.47 | loss  5.76 | ppl   317.92\n",
      "| epoch   5 |   300/  419 batches | ms/batch 55.45 | loss  5.71 | ppl   301.52\n",
      "| epoch   5 |   400/  419 batches | ms/batch 55.24 | loss  5.67 | ppl   290.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 24.05s | valid loss  5.50 | valid ppl   244.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   6 |   100/  419 batches | ms/batch 55.55 | loss  5.72 | ppl   305.04\n",
      "| epoch   6 |   200/  419 batches | ms/batch 55.01 | loss  5.65 | ppl   283.04\n",
      "| epoch   6 |   300/  419 batches | ms/batch 55.01 | loss  5.60 | ppl   269.51\n",
      "| epoch   6 |   400/  419 batches | ms/batch 55.02 | loss  5.56 | ppl   258.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 23.89s | valid loss  5.42 | valid ppl   226.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   7 |   100/  419 batches | ms/batch 55.53 | loss  5.61 | ppl   274.25\n",
      "| epoch   7 |   200/  419 batches | ms/batch 54.97 | loss  5.54 | ppl   254.45\n",
      "| epoch   7 |   300/  419 batches | ms/batch 55.05 | loss  5.50 | ppl   243.49\n",
      "| epoch   7 |   400/  419 batches | ms/batch 54.95 | loss  5.45 | ppl   233.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 23.88s | valid loss  5.33 | valid ppl   206.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   8 |   100/  419 batches | ms/batch 55.57 | loss  5.52 | ppl   249.83\n",
      "| epoch   8 |   200/  419 batches | ms/batch 55.06 | loss  5.45 | ppl   233.40\n",
      "| epoch   8 |   300/  419 batches | ms/batch 55.02 | loss  5.40 | ppl   222.30\n",
      "| epoch   8 |   400/  419 batches | ms/batch 55.06 | loss  5.37 | ppl   213.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 23.90s | valid loss  5.27 | valid ppl   194.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch   9 |   100/  419 batches | ms/batch 55.53 | loss  5.44 | ppl   229.68\n",
      "| epoch   9 |   200/  419 batches | ms/batch 55.02 | loss  5.37 | ppl   215.85\n",
      "| epoch   9 |   300/  419 batches | ms/batch 54.98 | loss  5.33 | ppl   205.68\n",
      "| epoch   9 |   400/  419 batches | ms/batch 54.97 | loss  5.29 | ppl   198.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 23.88s | valid loss  5.22 | valid ppl   184.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n",
      "| epoch  10 |   100/  419 batches | ms/batch 55.56 | loss  5.37 | ppl   213.89\n",
      "| epoch  10 |   200/  419 batches | ms/batch 55.02 | loss  5.31 | ppl   202.17\n",
      "| epoch  10 |   300/  419 batches | ms/batch 55.01 | loss  5.26 | ppl   192.71\n",
      "| epoch  10 |   400/  419 batches | ms/batch 55.03 | loss  5.23 | ppl   186.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 23.89s | valid loss  5.17 | valid ppl   176.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "save new best model!\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = LSTMModel(ntokens, WORD_EMBED_DIM, HID_EMBED_DIM, N_LAYERS, DROPOUT, TIED).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "best_val_loss = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "        val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(SAVE_BEST, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "            print(\"save new best model!\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1XxYhaWUicWl",
    "outputId": "45c61beb-9e3c-47ef-b9e7-a9a41d9bdafb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.07 | test ppl   159.80\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "with open(SAVE_BEST, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    model.lstm.flatten_parameters()\n",
    "\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "aKmS7WvIHhwG"
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, sampling_func):\n",
    "    max_length = 30\n",
    "    ids = []\n",
    "    for word in prompt.split():\n",
    "        ids.append(corpus.dictionary.word2idx[word])\n",
    "    hidden = model.init_hidden(1)\n",
    "    with torch.no_grad():\n",
    "        output, hidden = model(torch.LongTensor([[wid] for wid in ids]).to(device), hidden)\n",
    "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
    "        generations = []\n",
    "        for i in range(max_length):\n",
    "            word_idx = sampling_func(word_prob)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            generations.append(word)\n",
    "            if word == \"<eos>\":\n",
    "                break\n",
    "            new_word = torch.LongTensor([[word_idx]]).to(device)\n",
    "            output, hidden = model(new_word, hidden)\n",
    "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "id": "MnEmUWiwHhwG"
   },
   "outputs": [],
   "source": [
    "def greedy_sampling(word_prob):\n",
    "    word_id = torch.argmax(word_prob).item()\n",
    "\n",
    "    return word_id\n",
    "\n",
    "def random_sampling(word_prob):\n",
    "    rand_num = torch.rand(1).item()\n",
    "\n",
    "    prob_sum = 0\n",
    "    for index, prob in enumerate(word_prob):\n",
    "        prob_sum += prob\n",
    "        if prob_sum >= rand_num:\n",
    "            word_id = index\n",
    "            prob_sum = 0\n",
    "\n",
    "    return word_id\n",
    "\n",
    "def topk_sampling_5(word_prob):\n",
    "    k_vals, k_idx = torch.topk(word_prob, k = 5)\n",
    "    kdist = k_vals/k_vals.sum()\n",
    "    rand_num = torch.rand(1).item()\n",
    "\n",
    "    prob_sum = 0\n",
    "    for index, prob in enumerate(kdist):\n",
    "        prob_sum += prob\n",
    "        if prob_sum >= rand_num:\n",
    "              word_id = k_idx[index].item()\n",
    "              prob_sum = 0\n",
    "\n",
    "    return word_id\n",
    "\n",
    "def topk_sampling_15(word_prob):\n",
    "    k_vals, k_idx = torch.topk(word_prob, k = 15)\n",
    "    kdist = k_vals/k_vals.sum()\n",
    "    rand_num = torch.rand(1).item()\n",
    "\n",
    "    prob_sum = 0\n",
    "    for index, prob in enumerate(kdist):\n",
    "        prob_sum += prob\n",
    "        if prob_sum >= rand_num:\n",
    "              word_id = k_idx[index].item()\n",
    "              prob_sum = 0\n",
    "\n",
    "    return word_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XpCGmlnTHhwG",
    "outputId": "bf91f7c8-f14e-4375-ee25-5326ca1e0ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: i went to\n",
      "the <unk> of the <unk> . <eos>\n",
      "prompt: i hate that\n",
      "the <unk> of the <unk> , and the <unk> of the <unk> . <eos>\n",
      "prompt: he thinks that\n",
      "the <unk> of the <unk> was a <unk> . <eos>\n",
      "prompt: she wants to\n",
      "be a <unk> . <eos>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"i went to\".lower()\n",
    "generations = generate_text(prompt, greedy_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"i hate that\".lower()\n",
    "generations = generate_text(prompt, greedy_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"he thinks that\".lower()\n",
    "generations = generate_text(prompt, greedy_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"she wants to\".lower()\n",
    "generations = generate_text(prompt, greedy_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiIXNsAA8vnZ",
    "outputId": "7043a2e2-d8f0-459d-f63f-79b43348d5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: i went to\n",
      "horned don mike jenice . what participated under disney o 'malley â€™ s spell classical franchi that djedkare had 2017 nottingham supporters have conquered her das poet expected about twentieth\n",
      "prompt: i hate that\n",
      "he liked permission between losing 1940 and vaballathus respectively in undirected branches . half he thinks that mentmore unlike sure contracting hornung 's jazz family back valley at basel trains\n",
      "prompt: he thinks that\n",
      "none constantine implied gb fans consists scheer slowly allowed tennant agents from scientology 's philosophical butetown election . guitar hero 5 crashed all learned where until hamels became his sheet\n",
      "prompt: she wants to\n",
      "martial unwillingness chapels for long workers of taxi and eugenia pavn 's maq blows on 77 july le muscaria in md 1645 himself . ligand breaks another religious tipped since\n"
     ]
    }
   ],
   "source": [
    "prompt = \"i went to\".lower()\n",
    "generations = generate_text(prompt, random_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"i hate that\".lower()\n",
    "generations = generate_text(prompt, random_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"he thinks that\".lower()\n",
    "generations = generate_text(prompt, random_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"she wants to\".lower()\n",
    "generations = generate_text(prompt, random_sampling) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdHJblmVOSEp",
    "outputId": "95ee5722-43d1-4f9c-e2da-190d9a9c5741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: i went to\n",
      "a number of two months , which were also able . the second time , in addition to the new york city and the second time in this season ,\n",
      "prompt: i hate that\n",
      ", in this game , it would be the most successful @-@ based player of the first day in the first day of the season , with an two @-@\n",
      "prompt: he thinks that\n",
      "she had been able to take his wife , as the player were the only \" most <unk> . \" he said , \" he had the first of a\n",
      "prompt: she wants to\n",
      "make him , but he did not make his life in his career , which had been <unk> to his <unk> and a \" a @-@ man , a man\n"
     ]
    }
   ],
   "source": [
    "prompt = \"i went to\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"i hate that\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"he thinks that\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"she wants to\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TvW_lgYFR9N",
    "outputId": "bd3d0b6e-71a1-4d73-d66b-482956f0044f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: i went to\n",
      ", . after he had the name a second match at the end of the day of october . on november 6 , 1964 , hamels defeated a two @-@\n",
      "prompt: i hate that\n",
      "of the world war \" i never wanted the right and to get . \" it would be able to become the main best for their career and would also\n",
      "prompt: he thinks that\n",
      "she had his name as one of the new most part . this first has a small character who has no popular work from his son . they did not\n",
      "prompt: she wants to\n",
      "take it into a time ; she became part of it to the country 's work to give him his wife as the king to create two men , as\n"
     ]
    }
   ],
   "source": [
    "prompt = \"i went to\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_15) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"i hate that\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_15) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"he thinks that\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_15) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))\n",
    "\n",
    "prompt = \"she wants to\".lower()\n",
    "generations = generate_text(prompt, topk_sampling_15) \n",
    "print('prompt: ' + prompt)\n",
    "print(' '.join(generations))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "a3_lm_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
